import gradio as gr
import time
import numpy as np

# æ¨¡æ‹Ÿçš„æ¨¡å‹é€‰é¡¹
END_TO_END_MODELS = [
    "Qwen2.5-Omni-3B",
    "Qwen2.5-Omni-7B",
    "Qwen3-Omni-30B-A3B-Instruct", 
    "Qwen3-Omni-30B-A3B-Thinking"
]

SEPARATED_MODELS = {
    "ASR": ["Whisper", "Wav2Vec2", "Conformer"],
    "LLM": ["Qwen3-4B-Instruct", "Qwen3-8B"],
    "TTS": ["Bark", "VITS", "Tacotron2"]
}

def process_audio(audio_input, conversation_mode, system_prompt, language, speed, emotion,
                  end_to_end_model, asr_model, llm_model, tts_model, text_input, image_input, video_input):
    """
    å¤„ç†éŸ³é¢‘è¾“å…¥å¹¶è¿”å›å“åº”
    """
    if conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¾“å…¥
        has_input = any([audio_input, text_input, image_input, video_input])
        if not has_input:
            return None, "", ""
    else:
        if audio_input is None:
            return None, "", ""
    
    # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
    time.sleep(1)
    
    # æ„å»ºè¾“å…¥æè¿°
    input_desc = []
    if audio_input:
        input_desc.append("è¯­éŸ³")
    if text_input:
        input_desc.append("æ–‡æœ¬")
    if image_input:
        input_desc.append("å›¾åƒ")
    if video_input:
        input_desc.append("è§†é¢‘")
    
    input_types = "ã€".join(input_desc) if input_desc else "éŸ³é¢‘"
    
    # æ¨¡æ‹Ÿè½¬å½•ç»“æœ
    transcription = f"[æ¨¡æ‹Ÿè½¬å½•] ç”¨æˆ·é€šè¿‡{input_types}è¾“å…¥ä¿¡æ¯"
    
    # æ„å»ºå½“å‰é…ç½®ä¿¡æ¯
    current_config = ""
    if conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
        current_config = f"- æ¨¡å‹: {end_to_end_model}"
    else:
        current_config = f"- ASRæ¨¡å‹: {asr_model}\n- LLMæ¨¡å‹: {llm_model}\n- TTSæ¨¡å‹: {tts_model}"
    
    # æ¨¡æ‹ŸAIå“åº”
    ai_response = f"ä½ å¥½ï¼æˆ‘å·²ç»æ”¶åˆ°ä½ çš„{input_types}ä¿¡æ¯ã€‚å½“å‰è®¾ç½®:\n- æ¨¡å¼: {conversation_mode}\n{current_config}\n- è¯­è¨€: {language}\n- è¯­é€Ÿ: {speed}\n- æƒ…æ„Ÿ: {emotion}"
    
    # æ¨¡æ‹Ÿç”Ÿæˆçš„å“åº”éŸ³é¢‘
    sample_rate = 22050
    duration = 2
    t = np.linspace(0, duration, int(sample_rate * duration))
    audio_data = np.sin(2 * np.pi * 440 * t)
    
    return (sample_rate, audio_data), transcription, ai_response

def update_input_interface(mode):
    """æ ¹æ®å¯¹è¯æ¨¡å¼æ›´æ–°è¾“å…¥ç•Œé¢"""
    if mode == "å…¨æ¨¡æ€æ¨¡å‹":
        return [
            gr.update(visible=True),   # å¤šæ¨¡æ€è¾“å…¥ç»„
            gr.update(visible=False)   # ä»…éŸ³é¢‘è¾“å…¥
        ]
    else:
        return [
            gr.update(visible=False),  # å¤šæ¨¡æ€è¾“å…¥ç»„
            gr.update(visible=True)    # ä»…éŸ³é¢‘è¾“å…¥
        ]

def update_model_selection(mode):
    """æ ¹æ®é€‰æ‹©çš„å¯¹è¯æ¨¡å¼æ›´æ–°æ¨¡å‹é€‰æ‹©ç»„ä»¶"""
    if mode == "å…¨æ¨¡æ€æ¨¡å‹":
        return [
            gr.update(visible=True),
            gr.update(visible=False),
            gr.update(visible=False),
            gr.update(visible=False)
        ]
    else:
        return [
            gr.update(visible=False),
            gr.update(visible=True),
            gr.update(visible=True),
            gr.update(visible=True)
        ]

def save_system_prompt(prompt):
    """ä¿å­˜ç³»ç»Ÿæç¤ºè¯"""
    return f"ç³»ç»Ÿæç¤ºè¯å·²ä¿å­˜: {prompt[:50]}..." if len(prompt) > 50 else f"ç³»ç»Ÿæç¤ºè¯å·²ä¿å­˜: {prompt}"

with gr.Blocks(title="å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿ") as demo:
    gr.Markdown("# ğŸ™ï¸ å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿ")
    
    with gr.Tab("ä¸»é¡µ"):
        with gr.Row():
            # å·¦ä¾§åŒºåŸŸ - è¯­éŸ³äº¤äº’ç•Œé¢
            with gr.Column(scale=2):
                # å¯¹è¯æ¨¡å¼é€‰æ‹©
                conversation_mode = gr.Radio(
                    choices=["å…¨æ¨¡æ€æ¨¡å‹", "äº¤äº’å¼è¯­éŸ³"],
                    value="å…¨æ¨¡æ€æ¨¡å‹",
                    label="å¯¹è¯æ¨¡å¼"
                )
                
                # å¤šæ¨¡æ€è¾“å…¥åŒºåŸŸï¼ˆå…¨æ¨¡æ€æ¨¡å‹æ—¶æ˜¾ç¤ºï¼‰
                with gr.Group(visible=True) as multimodal_input_group:
                    gr.Markdown("### ğŸ“¥ å¤šæ¨¡æ€è¾“å…¥")
                    # ç¬¬ä¸€æ’ï¼šå›¾åƒå’Œè§†é¢‘è¾“å…¥
                    with gr.Row():
                        with gr.Column():
                            image_input = gr.Image(
                                type="filepath",
                                label="ğŸ–¼ï¸ å›¾åƒ"
                            )
                        with gr.Column():
                            video_input = gr.Video(
                                label="ğŸ¬ è§†é¢‘"
                            )
                    
                    # ç¬¬äºŒæ’ï¼šæ–‡æœ¬è¾“å…¥
                    with gr.Row():
                        text_input = gr.Textbox(
                            label="ğŸ“ æ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥æ–‡æœ¬..."
                        )
                    
                    # ç¬¬ä¸‰æ’ï¼šéŸ³é¢‘è¾“å…¥
                    with gr.Row():
                        audio_input_multi = gr.Audio(
                            sources=["microphone"],
                            type="filepath",
                            label="ğŸ¤ è¯­éŸ³"
                        )
                
                # ä»…éŸ³é¢‘è¾“å…¥åŒºåŸŸï¼ˆäº¤äº’å¼è¯­éŸ³æ—¶æ˜¾ç¤ºï¼‰
                with gr.Group(visible=False) as audio_only_group:
                    gr.Markdown("### ğŸ¤ éŸ³é¢‘è¾“å…¥")
                    audio_input_single = gr.Audio(
                        sources=["microphone"],
                        type="filepath",
                        label="ç‚¹å‡»å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘"
                    )
                
                # è¾“å‡ºåŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### ğŸ“¤ AIå›åº”")
                    audio_output = gr.Audio(
                        label="ğŸ”Š è¯­éŸ³å›åº”",
                        autoplay=True
                    )
                
                # è½¬å½•å’Œå“åº”æ˜¾ç¤º
                with gr.Row():
                    with gr.Column():
                        transcription_output = gr.Textbox(
                            label="ğŸ“ è½¬æ–‡å­—ç»“æœ",
                            interactive=False
                        )
                    with gr.Column():
                        response_output = gr.Textbox(
                            label="ğŸ¤– æ–‡æœ¬å›åº”",
                            interactive=False
                        )
                
                # æäº¤æŒ‰é’®
                submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", elem_classes=["send-button"])
            
            # å³ä¾§åŒºåŸŸ - é…ç½®ç•Œé¢
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ¤– æ¨¡å‹é€‰æ‹©")
                    
                    # å…¨æ¨¡æ€æ¨¡å‹é€‰æ‹©
                    end_to_end_model = gr.Dropdown(
                        choices=END_TO_END_MODELS,
                        value=END_TO_END_MODELS[0],
                        label="å…¨æ¨¡æ€æ¨¡å‹",
                        visible=True
                    )
                    
                    # åˆ†ç¦»å¼æ¨¡å‹é€‰æ‹©
                    asr_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["ASR"],
                        value=SEPARATED_MODELS["ASR"][0],
                        label="ASRæ¨¡å‹",
                        visible=False
                    )
                    
                    llm_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["LLM"],
                        value=SEPARATED_MODELS["LLM"][0],
                        label="LLMæ¨¡å‹",
                        visible=False
                    )
                    
                    tts_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["TTS"],
                        value=SEPARATED_MODELS["TTS"][0],
                        label="TTSæ¨¡å‹",
                        visible=False
                    )
                
                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç³»ç»Ÿé…ç½®")
                    
                    system_prompt = gr.Textbox(
                        value="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨å‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”å›ç­”é—®é¢˜ã€‚",
                        label="ç³»ç»Ÿæç¤ºè¯",
                        lines=3
                    )
                    
                    # æ·»åŠ ä¿å­˜æŒ‰é’®
                    with gr.Row():
                        save_prompt_btn = gr.Button("ğŸ’¾ ä¿å­˜", size="sm")
                        save_status = gr.Textbox(label="", interactive=False, visible=False)
                    
                    language = gr.Dropdown(
                        choices=["ä¸­æ–‡", "English", "æ—¥æœ¬èª", "FranÃ§ais", "EspaÃ±ol"],
                        value="ä¸­æ–‡",
                        label="è¯­è¨€"
                    )
                    
                    with gr.Row():
                        speed = gr.Slider(
                            minimum=0.5,
                            maximum=2.0,
                            value=1.0,
                            step=0.1,
                            label="è¯­é€Ÿ"
                        )
                        
                        emotion = gr.Dropdown(
                            choices=["é»˜è®¤", "å¼€å¿ƒ", "ä¸¥è‚ƒ", "æ¸©æŸ”", "å…´å¥‹"],
                            value="é»˜è®¤",
                            label="æƒ…æ„Ÿ"
                        )
    
    # äº‹ä»¶å¤„ç†
    conversation_mode.change(
        fn=update_input_interface,
        inputs=[conversation_mode],
        outputs=[multimodal_input_group, audio_only_group]
    )
    
    conversation_mode.change(
        fn=update_model_selection,
        inputs=[conversation_mode],
        outputs=[end_to_end_model, asr_model, llm_model, tts_model]
    )
    
    def get_active_inputs(conversation_mode, audio_multi, audio_single, text_inp, image_inp, video_inp):
        """æ ¹æ®å¯¹è¯æ¨¡å¼é€‰æ‹©æ´»åŠ¨çš„è¾“å…¥"""
        if conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
            return audio_multi, text_inp, image_inp, video_inp
        else:
            return audio_single, "", None, None
    
    def handle_submit(mode, audio_multi, audio_single, text_inp, image_inp, video_inp, 
                      system_prompt, language, speed, emotion, end_to_end_model, 
                      asr_model, llm_model, tts_model):
        """å¤„ç†æäº¤äº‹ä»¶çš„å‡½æ•°"""
        active_inputs = get_active_inputs(mode, audio_multi, audio_single, text_inp, image_inp, video_inp)
        return process_audio(*active_inputs, mode, system_prompt, language, speed, emotion,
                           end_to_end_model, asr_model, llm_model, tts_model)
    
    submit_btn.click(
        fn=handle_submit,
        inputs=[conversation_mode, audio_input_multi, audio_input_single, text_input, image_input, video_input,
                system_prompt, language, speed, emotion, end_to_end_model, asr_model, llm_model, tts_model],
        outputs=[audio_output, transcription_output, response_output]
    )
    
    # ä¿å­˜ç³»ç»Ÿæç¤ºè¯
    save_prompt_btn.click(
        fn=save_system_prompt,
        inputs=[system_prompt],
        outputs=[save_status]
    )

if __name__ == "__main__":
    demo.launch()