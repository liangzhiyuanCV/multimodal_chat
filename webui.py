import time
from dataclasses import dataclass
from typing import Optional, Any

import gradio as gr
import numpy as np

@dataclass
class AudioProcessParams:
    """
    éŸ³é¢‘å¤„ç†å‚æ•°å°è£…ç±»
    
    ç”¨äºå°è£…å¤„ç†éŸ³é¢‘è¾“å…¥æ‰€éœ€çš„æ‰€æœ‰å‚æ•°ï¼Œé¿å…å‡½æ•°å‚æ•°åˆ—è¡¨è¿‡é•¿çš„é—®é¢˜ã€‚
    """
    # è¾“å…¥ç›¸å…³å‚æ•°
    audio_input: Optional[str] = None           # éŸ³é¢‘è¾“å…¥æ–‡ä»¶è·¯å¾„
    text_input: Optional[str] = None            # æ–‡æœ¬è¾“å…¥å†…å®¹
    image_input: Optional[str] = None           # å›¾åƒè¾“å…¥æ–‡ä»¶è·¯å¾„
    video_input: Optional[str] = None           # è§†é¢‘è¾“å…¥æ–‡ä»¶è·¯å¾„
    
    # é…ç½®ç›¸å…³å‚æ•°
    conversation_mode: str = "å…¨æ¨¡æ€æ¨¡å‹"        # å¯¹è¯æ¨¡å¼ï¼ˆå…¨æ¨¡æ€æ¨¡å‹/äº¤äº’å¼è¯­éŸ³ï¼‰
    system_prompt: str = ""                     # ç³»ç»Ÿæç¤ºè¯
    language: str = "ä¸­æ–‡"                      # è¯­è¨€è®¾ç½®
    speed: float = 1.0                          # è¯­é€Ÿæ§åˆ¶
    emotion: str = "é»˜è®¤"                       # æƒ…æ„Ÿè®¾ç½®
    
    # æ¨¡å‹é€‰æ‹©ç›¸å…³å‚æ•°
    end_to_end_model: str = ""                  # ç«¯åˆ°ç«¯æ¨¡å‹é€‰æ‹©
    asr_model: str = ""                         # ASRæ¨¡å‹é€‰æ‹©
    llm_model: str = ""                         # LLMæ¨¡å‹é€‰æ‹©
    tts_model: str = ""                         # TTSæ¨¡å‹é€‰æ‹©

# æ¨¡æ‹Ÿçš„ç«¯åˆ°ç«¯æ¨¡å‹é€‰é¡¹
END_TO_END_MODELS = [
    "Qwen2.5-Omni-3B",
    "Qwen2.5-Omni-7B",
    "Qwen3-Omni-30B-A3B-Instruct", 
    "Qwen3-Omni-30B-A3B-Thinking"
]

# æ¨¡æ‹Ÿçš„åˆ†ç¦»å¼æ¨¡å‹é€‰é¡¹
SEPARATED_MODELS = {
    "ASR": ["Whisper", "Wav2Vec2", "Conformer"],      # è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹
    "LLM": ["Qwen3-4B-Instruct", "Qwen3-8B"],         # å¤§è¯­è¨€æ¨¡å‹
    "TTS": ["Bark", "VITS", "Tacotron2"]              # æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹
}

def process_audio(params: AudioProcessParams):
    """
    å¤„ç†éŸ³é¢‘è¾“å…¥å¹¶è¿”å›å“åº”
    
    æ ¹æ®ä¸åŒçš„å¯¹è¯æ¨¡å¼å¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆéŸ³é¢‘ã€æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘ç­‰ï¼‰ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„AIå“åº”ã€‚
    
    Args:
        params (AudioProcessParams): å°è£…äº†æ‰€æœ‰å¤„ç†æ‰€éœ€å‚æ•°çš„å¯¹è±¡
        
    Returns:
        tuple: åŒ…å«ä¸‰ä¸ªå…ƒç´ çš„å…ƒç»„:
            - éŸ³é¢‘è¾“å‡º: (é‡‡æ ·ç‡, éŸ³é¢‘æ•°æ®) æˆ– None
            - è½¬å½•æ–‡æœ¬: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬è¡¨ç¤º
            - AIå“åº”: AIç”Ÿæˆçš„æ–‡æœ¬å“åº”
    """
    # æ ¹æ®å¯¹è¯æ¨¡å¼æ£€æŸ¥è¾“å…¥æœ‰æ•ˆæ€§
    if params.conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
        # å…¨æ¨¡æ€æ¨¡å‹æ¨¡å¼ä¸‹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è¾“å…¥
        has_input = any([params.audio_input, params.text_input, params.image_input, params.video_input])
        if not has_input:
            return None, "", ""
    else:
        # äº¤äº’å¼è¯­éŸ³æ¨¡å¼ä¸‹ï¼Œå¿…é¡»æœ‰éŸ³é¢‘è¾“å…¥
        if params.audio_input is None:
            return None, "", ""
    
    # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹ï¼ˆæ·»åŠ 1ç§’å»¶è¿Ÿï¼‰
    time.sleep(1)
    
    # æ„å»ºè¾“å…¥æè¿°ï¼Œç”¨äºæ˜¾ç¤ºç”¨æˆ·é€šè¿‡å“ªäº›æ–¹å¼è¾“å…¥äº†ä¿¡æ¯
    input_desc = []
    if params.audio_input:
        input_desc.append("è¯­éŸ³")
    if params.text_input:
        input_desc.append("æ–‡æœ¬")
    if params.image_input:
        input_desc.append("å›¾åƒ")
    if params.video_input:
        input_desc.append("è§†é¢‘")
    
    # æ ¼å¼åŒ–è¾“å…¥ç±»å‹æè¿°
    input_types = "ã€".join(input_desc) if input_desc else "éŸ³é¢‘"
    
    # æ¨¡æ‹Ÿè½¬å½•ç»“æœ
    transcription = f"[æ¨¡æ‹Ÿè½¬å½•] ç”¨æˆ·é€šè¿‡{input_types}è¾“å…¥ä¿¡æ¯"
    
    # æ ¹æ®å¯¹è¯æ¨¡å¼æ„å»ºå½“å‰é…ç½®ä¿¡æ¯
    current_config = ""
    if params.conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
        # å…¨æ¨¡æ€æ¨¡å‹æ¨¡å¼ä¸‹æ˜¾ç¤ºç«¯åˆ°ç«¯æ¨¡å‹
        current_config = f"- æ¨¡å‹: {params.end_to_end_model}"
    else:
        # äº¤äº’å¼è¯­éŸ³æ¨¡å¼ä¸‹æ˜¾ç¤ºåˆ†ç¦»å¼æ¨¡å‹ç»„åˆ
        current_config = f"- ASRæ¨¡å‹: {params.asr_model}\n- LLMæ¨¡å‹: {params.llm_model}\n- TTSæ¨¡å‹: {params.tts_model}"
    
    # æ„å»ºAIå“åº”æ–‡æœ¬ï¼ŒåŒ…å«å½“å‰è®¾ç½®ä¿¡æ¯
    ai_response = f"ä½ å¥½ï¼æˆ‘å·²ç»æ”¶åˆ°ä½ çš„{input_types}ä¿¡æ¯ã€‚å½“å‰è®¾ç½®:\n- æ¨¡å¼: {params.conversation_mode}\n{current_config}\n- è¯­è¨€: {params.language}\n- è¯­é€Ÿ: {params.speed}\n- æƒ…æ„Ÿ: {params.emotion}"
    
    # æ¨¡æ‹Ÿç”Ÿæˆå“åº”éŸ³é¢‘ï¼ˆ440Hzæ­£å¼¦æ³¢ï¼ŒæŒç»­2ç§’ï¼‰
    sample_rate = 22050
    duration = 2
    t = np.linspace(0, duration, int(sample_rate * duration))
    audio_data = np.sin(2 * np.pi * 440 * t)
    
    return (sample_rate, audio_data), transcription, ai_response

def update_input_interface(mode):
    """
    æ ¹æ®å¯¹è¯æ¨¡å¼æ›´æ–°è¾“å…¥ç•Œé¢çš„å¯è§æ€§
    
    åœ¨å…¨æ¨¡æ€æ¨¡å‹å’Œäº¤äº’å¼è¯­éŸ³æ¨¡å¼ä¹‹é—´åˆ‡æ¢æ—¶ï¼Œæ§åˆ¶ä¸åŒè¾“å…¥ç»„ä»¶çš„æ˜¾ç¤ºçŠ¶æ€ã€‚
    
    Args:
        mode (str): å½“å‰å¯¹è¯æ¨¡å¼ï¼Œå¯é€‰å€¼ä¸º"å…¨æ¨¡æ€æ¨¡å‹"æˆ–"äº¤äº’å¼è¯­éŸ³"
        
    Returns:
        list: åŒ…å«ä¸¤ä¸ªgr.updateå¯¹è±¡çš„åˆ—è¡¨ï¼Œåˆ†åˆ«æ§åˆ¶å¤šæ¨¡æ€è¾“å…¥ç»„å’Œä»…éŸ³é¢‘è¾“å…¥ç»„çš„å¯è§æ€§
    """
    if mode == "å…¨æ¨¡æ€æ¨¡å‹":
        # å…¨æ¨¡æ€æ¨¡å‹æ¨¡å¼ï¼šæ˜¾ç¤ºå¤šæ¨¡æ€è¾“å…¥ç»„ï¼Œéšè—ä»…éŸ³é¢‘è¾“å…¥ç»„
        return [
            gr.update(visible=True),   # å¤šæ¨¡æ€è¾“å…¥ç»„
            gr.update(visible=False)   # ä»…éŸ³é¢‘è¾“å…¥
        ]
    else:
        # äº¤äº’å¼è¯­éŸ³æ¨¡å¼ï¼šéšè—å¤šæ¨¡æ€è¾“å…¥ç»„ï¼Œæ˜¾ç¤ºä»…éŸ³é¢‘è¾“å…¥ç»„
        return [
            gr.update(visible=False),  # å¤šæ¨¡æ€è¾“å…¥ç»„
            gr.update(visible=True)    # ä»…éŸ³é¢‘è¾“å…¥
        ]

def update_model_selection(mode):
    """
    æ ¹æ®é€‰æ‹©çš„å¯¹è¯æ¨¡å¼æ›´æ–°æ¨¡å‹é€‰æ‹©ç»„ä»¶çš„å¯è§æ€§
    
    åœ¨ä¸åŒå¯¹è¯æ¨¡å¼ä¸‹ï¼Œæ˜¾ç¤ºç›¸åº”çš„æ¨¡å‹é€‰æ‹©ç»„ä»¶ã€‚
    
    Args:
        mode (str): å½“å‰å¯¹è¯æ¨¡å¼ï¼Œå¯é€‰å€¼ä¸º"å…¨æ¨¡æ€æ¨¡å‹"æˆ–"äº¤äº’å¼è¯­éŸ³"
        
    Returns:
        list: åŒ…å«å››ä¸ªgr.updateå¯¹è±¡çš„åˆ—è¡¨ï¼Œåˆ†åˆ«æ§åˆ¶ç«¯åˆ°ç«¯æ¨¡å‹å’Œä¸‰ä¸ªåˆ†ç¦»å¼æ¨¡å‹ç»„ä»¶çš„å¯è§æ€§
    """
    if mode == "å…¨æ¨¡æ€æ¨¡å‹":
        # å…¨æ¨¡æ€æ¨¡å‹æ¨¡å¼ï¼šæ˜¾ç¤ºç«¯åˆ°ç«¯æ¨¡å‹é€‰æ‹©ï¼Œéšè—åˆ†ç¦»å¼æ¨¡å‹é€‰æ‹©
        return [
            gr.update(visible=True),   # ç«¯åˆ°ç«¯æ¨¡å‹
            gr.update(visible=False),  # ASRæ¨¡å‹
            gr.update(visible=False),  # LLMæ¨¡å‹
            gr.update(visible=False)   # TTSæ¨¡å‹
        ]
    else:
        # äº¤äº’å¼è¯­éŸ³æ¨¡å¼ï¼šéšè—ç«¯åˆ°ç«¯æ¨¡å‹é€‰æ‹©ï¼Œæ˜¾ç¤ºåˆ†ç¦»å¼æ¨¡å‹é€‰æ‹©
        return [
            gr.update(visible=False),  # ç«¯åˆ°ç«¯æ¨¡å‹
            gr.update(visible=True),   # ASRæ¨¡å‹
            gr.update(visible=True),   # LLMæ¨¡å‹
            gr.update(visible=True)    # TTSæ¨¡å‹
        ]

def save_system_prompt(prompt):
    """
    ä¿å­˜ç³»ç»Ÿæç¤ºè¯
    
    ä¿å­˜ç”¨æˆ·è®¾ç½®çš„ç³»ç»Ÿæç¤ºè¯ï¼Œå¹¶è¿”å›ä¿å­˜çŠ¶æ€ä¿¡æ¯ã€‚
    
    Args:
        prompt (str): ç”¨æˆ·è¾“å…¥çš„ç³»ç»Ÿæç¤ºè¯
        
    Returns:
        str: ä¿å­˜çŠ¶æ€ä¿¡æ¯ï¼Œå¦‚æœæç¤ºè¯è¿‡é•¿åˆ™æˆªæ–­æ˜¾ç¤º
    """
    return f"ç³»ç»Ÿæç¤ºè¯å·²ä¿å­˜: {prompt[:50]}..." if len(prompt) > 50 else f"ç³»ç»Ÿæç¤ºè¯å·²ä¿å­˜: {prompt}"

with gr.Blocks(title="å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿ") as demo:
    gr.Markdown("# ğŸ™ï¸ å®æ—¶è¯­éŸ³å¯¹è¯ç³»ç»Ÿ")
    
    with gr.Tab("ä¸»é¡µ"):
        with gr.Row():
            # å·¦ä¾§åŒºåŸŸ - è¯­éŸ³äº¤äº’ç•Œé¢
            with gr.Column(scale=2):
                # å¯¹è¯æ¨¡å¼é€‰æ‹©
                conversation_mode = gr.Radio(
                    choices=["å…¨æ¨¡æ€æ¨¡å‹", "äº¤äº’å¼è¯­éŸ³"],
                    value="å…¨æ¨¡æ€æ¨¡å‹",
                    label="å¯¹è¯æ¨¡å¼"
                )
                
                # å¤šæ¨¡æ€è¾“å…¥åŒºåŸŸï¼ˆå…¨æ¨¡æ€æ¨¡å‹æ—¶æ˜¾ç¤ºï¼‰
                with gr.Group(visible=True) as multimodal_input_group:
                    gr.Markdown("### ğŸ“¥ å¤šæ¨¡æ€è¾“å…¥")
                    # ç¬¬ä¸€æ’ï¼šå›¾åƒå’Œè§†é¢‘è¾“å…¥
                    with gr.Row():
                        with gr.Column():
                            image_input = gr.Image(
                                type="filepath",
                                label="ğŸ–¼ï¸ å›¾åƒ"
                            )
                        with gr.Column():
                            video_input = gr.Video(
                                label="ğŸ¬ è§†é¢‘"
                            )
                    
                    # ç¬¬äºŒæ’ï¼šæ–‡æœ¬è¾“å…¥
                    with gr.Row():
                        text_input = gr.Textbox(
                            label="ğŸ“ æ–‡æœ¬",
                            placeholder="è¯·è¾“å…¥æ–‡æœ¬..."
                        )
                    
                    # ç¬¬ä¸‰æ’ï¼šéŸ³é¢‘è¾“å…¥
                    with gr.Row():
                        audio_input_multi = gr.Audio(
                            sources=["microphone"],
                            type="filepath",
                            label="ğŸ¤ è¯­éŸ³"
                        )
                
                # ä»…éŸ³é¢‘è¾“å…¥åŒºåŸŸï¼ˆäº¤äº’å¼è¯­éŸ³æ—¶æ˜¾ç¤ºï¼‰
                with gr.Group(visible=False) as audio_only_group:
                    gr.Markdown("### ğŸ¤ éŸ³é¢‘è¾“å…¥")
                    audio_input_single = gr.Audio(
                        sources=["microphone"],
                        type="filepath",
                        label="ç‚¹å‡»å½•éŸ³æˆ–ä¸Šä¼ éŸ³é¢‘"
                    )
                
                # è¾“å‡ºåŒºåŸŸ
                with gr.Group():
                    gr.Markdown("### ğŸ“¤ AIå›åº”")
                    audio_output = gr.Audio(
                        label="ğŸ”Š è¯­éŸ³å›åº”",
                        autoplay=True
                    )
                
                # è½¬å½•å’Œå“åº”æ˜¾ç¤º
                with gr.Row():
                    with gr.Column():
                        transcription_output = gr.Textbox(
                            label="ğŸ“ è½¬æ–‡å­—ç»“æœ",
                            interactive=False
                        )
                    with gr.Column():
                        response_output = gr.Textbox(
                            label="ğŸ¤– æ–‡æœ¬å›åº”",
                            interactive=False
                        )
                
                # æäº¤æŒ‰é’®
                submit_btn = gr.Button("ğŸš€ å‘é€", variant="primary", elem_classes=["send-button"])
            
            # å³ä¾§åŒºåŸŸ - é…ç½®ç•Œé¢
            with gr.Column(scale=1):
                with gr.Group():
                    gr.Markdown("### ğŸ¤– æ¨¡å‹é€‰æ‹©")
                    
                    # å…¨æ¨¡æ€æ¨¡å‹é€‰æ‹©
                    end_to_end_model = gr.Dropdown(
                        choices=END_TO_END_MODELS,
                        value=END_TO_END_MODELS[0],
                        label="å…¨æ¨¡æ€æ¨¡å‹",
                        visible=True
                    )
                    
                    # åˆ†ç¦»å¼æ¨¡å‹é€‰æ‹©
                    asr_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["ASR"],
                        value=SEPARATED_MODELS["ASR"][0],
                        label="ASRæ¨¡å‹",
                        visible=False
                    )
                    
                    llm_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["LLM"],
                        value=SEPARATED_MODELS["LLM"][0],
                        label="LLMæ¨¡å‹",
                        visible=False
                    )
                    
                    tts_model = gr.Dropdown(
                        choices=SEPARATED_MODELS["TTS"],
                        value=SEPARATED_MODELS["TTS"][0],
                        label="TTSæ¨¡å‹",
                        visible=False
                    )
                
                with gr.Group():
                    gr.Markdown("### âš™ï¸ ç³»ç»Ÿé…ç½®")
                    
                    system_prompt = gr.Textbox(
                        value="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç”¨å‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”å›ç­”é—®é¢˜ã€‚",
                        label="ç³»ç»Ÿæç¤ºè¯",
                        lines=3
                    )
                    
                    # æ·»åŠ ä¿å­˜æŒ‰é’®
                    with gr.Row():
                        save_prompt_btn = gr.Button("ğŸ’¾ ä¿å­˜", size="sm")
                        save_status = gr.Textbox(label="", interactive=False, visible=False)
                    
                    language = gr.Dropdown(
                        choices=["ä¸­æ–‡", "English", "æ—¥æœ¬èª", "FranÃ§ais", "EspaÃ±ol"],
                        value="ä¸­æ–‡",
                        label="è¯­è¨€"
                    )
                    
                    with gr.Row():
                        speed = gr.Slider(
                            minimum=0.5,
                            maximum=2.0,
                            value=1.0,
                            step=0.1,
                            label="è¯­é€Ÿ"
                        )
                        
                        emotion = gr.Dropdown(
                            choices=["é»˜è®¤", "å¼€å¿ƒ", "ä¸¥è‚ƒ", "æ¸©æŸ”", "å…´å¥‹"],
                            value="é»˜è®¤",
                            label="æƒ…æ„Ÿ"
                        )
    
    # äº‹ä»¶å¤„ç†
    conversation_mode.change(
        fn=update_input_interface,
        inputs=[conversation_mode],
        outputs=[multimodal_input_group, audio_only_group]
    )
    
    conversation_mode.change(
        fn=update_model_selection,
        inputs=[conversation_mode],
        outputs=[end_to_end_model, asr_model, llm_model, tts_model]
    )
    
    def get_active_inputs(conversation_mode, audio_multi, audio_single, text_inp, image_inp, video_inp):
        """æ ¹æ®å¯¹è¯æ¨¡å¼é€‰æ‹©æ´»åŠ¨çš„è¾“å…¥"""
        if conversation_mode == "å…¨æ¨¡æ€æ¨¡å‹":
            return audio_multi, text_inp, image_inp, video_inp
        else:
            return audio_single, "", None, None
    
    def create_audio_params(mode, audio_multi, audio_single, text_inp, image_inp, video_inp,
                           system_prompt, language, speed, emotion, end_to_end_model,
                           asr_model, llm_model, tts_model):
        """åˆ›å»ºAudioProcessParamså®ä¾‹"""
        audio_input, text_input, image_input, video_input = get_active_inputs(
            mode, audio_multi, audio_single, text_inp, image_inp, video_inp
        )
        
        return AudioProcessParams(
            audio_input=audio_input,
            text_input=text_input,
            image_input=image_input,
            video_input=video_input,
            conversation_mode=mode,
            system_prompt=system_prompt,
            language=language,
            speed=speed,
            emotion=emotion,
            end_to_end_model=end_to_end_model,
            asr_model=asr_model,
            llm_model=llm_model,
            tts_model=tts_model
        )
    
    submit_btn.click(
        fn=lambda mode, audio_multi, audio_single, text_inp, image_inp, video_inp,
                system_prompt, language, speed, emotion, end_to_end_model,
                asr_model, llm_model, tts_model:
                process_audio(create_audio_params(mode, audio_multi, audio_single, text_inp, image_inp, video_inp,
                                                system_prompt, language, speed, emotion, end_to_end_model,
                                                asr_model, llm_model, tts_model)),
        inputs=[conversation_mode, audio_input_multi, audio_input_single, text_input, image_input, video_input,
                system_prompt, language, speed, emotion, end_to_end_model, asr_model, llm_model, tts_model],
        outputs=[audio_output, transcription_output, response_output]
    )
    
    # ä¿å­˜ç³»ç»Ÿæç¤ºè¯
    save_prompt_btn.click(
        fn=save_system_prompt,
        inputs=[system_prompt],
        outputs=[save_status]
    )

if __name__ == "__main__":
    demo.launch()